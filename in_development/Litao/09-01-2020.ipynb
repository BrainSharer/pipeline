{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "DIR = '/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/X/preps/CH1/downsampled_cropped'\n",
    "volume = []\n",
    "for filename in sorted(os.listdir(DIR)):\n",
    "    filepath = os.path.join(DIR, filename)\n",
    "    volume.append(io.imread(filepath))\n",
    "volume = np.array(volume)\n",
    "volume = np.swapaxes(volume, 0, 2)\n",
    "\n",
    "# Precomputed folder path\n",
    "precompute_path = os.path.join(os.path.expanduser('~'), f'image_test')\n",
    "\n",
    "# Voxel resolution in nanometer (how much nanometer each element in numpy array represent)\n",
    "resol = (452, 452, 20000)\n",
    "\n",
    "# Voxel offset\n",
    "offset = (0, 0, 0)\n",
    "\n",
    "# Layer type\n",
    "layer_type = 'image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "from utilities.SqlController import SqlController\n",
    "from utils import get_segment_properties\n",
    "\n",
    "DIR = os.path.join('./')\n",
    "animal = 'MD589'\n",
    "downsample_factor = 16\n",
    "all_structures = False\n",
    "\n",
    "volume_path = os.path.join(DIR, f'{animal}_annotations_down{downsample_factor}.npy')\n",
    "\n",
    "# Numpy array whose shape should match (x, y, z, channel) or just (x, y, z) if it's single channel\n",
    "with open(volume_path, 'rb') as file:\n",
    "    volume = np.load(file)\n",
    "print(volume.shape)\n",
    "\n",
    "# Precomputed folder path\n",
    "precompute_path = os.path.join(DIR, f'{animal}_annotations_down{downsample_factor}')\n",
    "\n",
    "# Voxel resolution in nanometer (how much nanometer each element in numpy array represent)\n",
    "resol = (14464, 14464, 20000)\n",
    "\n",
    "# Voxel offset\n",
    "offset = (0, 0, 0)\n",
    "\n",
    "# Layer type\n",
    "layer_type = 'segmentation'\n",
    "\n",
    "# segmentation properties in the format of [(number1, label1), (number2, label2) ...]\n",
    "# where number is an integer that is in the volume and label is a string that describes that segmenetation\n",
    "segment_properties = get_segment_properties(all_known=all_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want to preview the numpy array before converting to precomputed format\n",
    "import neuroglancer\n",
    "viewer = neuroglancer.Viewer()\n",
    "print(viewer)\n",
    "\n",
    "all_volume_layer = neuroglancer.SegmentationLayer(\n",
    "    source = neuroglancer.LocalVolume(\n",
    "        data=volume, \n",
    "        dimensions=neuroglancer.CoordinateSpace(names=['x', 'y', 'z'], units='nm', scales=resol), \n",
    "        voxel_offset=offset\n",
    "    ),\n",
    ")\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    s.layers.clear()\n",
    "    s.layers['all'] = all_volume_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/seung-lab/cloud-volume\n",
    "# Don't change all compress=False. It seems that Neuroglancer can only read with compress=False\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "cloudpath = f'file://{precompute_path}'\n",
    "info = CloudVolume.create_new_info(\n",
    "    num_channels = volume.shape[3] if len(volume.shape) > 3 else 1,\n",
    "    layer_type = layer_type,\n",
    "    data_type = str(volume.dtype), # Channel images might be 'uint8'\n",
    "    encoding = 'raw', # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution = resol, # Voxel scaling, units are in nanometers\n",
    "    voxel_offset = offset, # x,y,z offset in voxels from the origin\n",
    "    chunk_size = [64, 64, 64], # units are voxels\n",
    "    volume_size = volume.shape[:3], # e.g. a cubic millimeter dataset\n",
    ")\n",
    "vol = CloudVolume(cloudpath, mip=0, info=info, compress=False)\n",
    "vol.commit_info()\n",
    "vol[:, :, :] = volume[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vol.info['segment_properties'] = 'names'\n",
    "vol.commit_info()\n",
    "\n",
    "segment_properties_path = os.path.join(precompute_path, 'names')\n",
    "os.makedirs(segment_properties_path, exist_ok=True)\n",
    "\n",
    "info = {\n",
    "    \"@type\": \"neuroglancer_segment_properties\", \n",
    "    \"inline\": {\n",
    "        \"ids\": [str(number) for number, label in segmentation_properties],\n",
    "        \"properties\": [{\n",
    "            \"id\": \"label\", \n",
    "            \"description\": \"Name of structures\",\n",
    "            \"type\": \"label\",\n",
    "            \"values\": [str(label) for number, label in segmentation_properties]\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(segment_properties_path, 'info'), 'w') as file:\n",
    "    json.dump(info, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seung-lab/igneous\n",
    "from taskqueue import LocalTaskQueue\n",
    "import igneous.task_creation as tc\n",
    "\n",
    "tq = LocalTaskQueue(parallel=True)\n",
    "tasks = tc.create_downsampling_tasks(cloudpath, compress=False) # Downsample the volumes \n",
    "tq.insert(tasks)\n",
    "tq.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = tc.create_meshing_tasks(cloudpath, mip=0, compress=False) # The first phase of creating mesh\n",
    "tq.insert(tasks)\n",
    "tq.execute()\n",
    "\n",
    "# It should be able to incoporated to above tasks, but it will give a weird bug. Don't know the reason\n",
    "tasks = tc.create_mesh_manifest_tasks(cloudpath) # The second phase of creating mesh\n",
    "tq.insert(tasks)\n",
    "tq.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinybrain\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "volume = full_brain_volume_annotated\n",
    "factor = (2, 2, 1)\n",
    "volumes = tinybrain.downsample_segmentation(volume, factor=factor, num_mips=2, sparse=False)\n",
    "volumes.insert(0, volume)\n",
    "\n",
    "path = 'file:///net/birdstore/Active_Atlas_Data/data_root/pipeline_data/test'\n",
    "info = CloudVolume.create_new_info(\n",
    "    num_channels = 1,\n",
    "    layer_type = 'segmentation',\n",
    "    data_type = 'uint32', # Channel images might be 'uint8'\n",
    "    encoding = 'compressed_segmentation', # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution = [10000, 10000, 20000], # Voxel scaling, units are in nanometers\n",
    "    voxel_offset = [0, 0, 0], # x,y,z offset in voxels from the origin\n",
    "    chunk_size = [512, 512, 16], # units are voxels\n",
    "    volume_size = volume.shape, # e.g. a cubic millimeter dataset\n",
    ")\n",
    "vol = CloudVolume(path, info=info, compress=False, progress=False)\n",
    "\n",
    "for mip, volume in enumerate(volumes):\n",
    "    vol.add_scale(np.array(factor) ** mip)\n",
    "    vol.commit_info()\n",
    "    vol = CloudVolume(path, mip=mip, compress=False, progress=False)\n",
    "    vol[:, :, :] = volume[:, :, :]\n",
    "    \n",
    "vol.info['segment_properties'] = 'names'\n",
    "vol.commit_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
